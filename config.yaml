# ============================================================================
# Databricks Metadata Tool Configuration
# ============================================================================
#
# Authentication (via environment variables or .env file):
#   DATABRICKS_ACCOUNT_ID     - Your Databricks Account ID
#   DATABRICKS_CLIENT_ID      - Service Principal Client ID
#   DATABRICKS_CLIENT_SECRET  - Service Principal Secret
#
# ============================================================================

# Databricks Account
databricks:
  # Account ID (can also be set via DATABRICKS_ACCOUNT_ID env var)
  account_id: null
  
  # Cloud provider: azure, aws, gcp
  cloud: azure

# ============================================================================
# Filters
# ============================================================================
filters:
  # Catalogs to exclude from collection
  exclude_catalogs: ['system', 'samples']
  
  # Schemas to exclude from collection
  exclude_schemas: ['information_schema']
  
  # Only collect tables from these catalogs (null = all catalogs)
  # Can also use CLI: --only-catalogs catalog1 catalog2
  only_catalogs: null

# ============================================================================
# Output
# ============================================================================
output:
  # Local output directory
  directory: "./outputs"

# Volume upload settings (when using --write-to-volume)
volume:
  catalog: "collection_catalog"
  schema: "collection_schema"
  volume: "collection_volume"

# ============================================================================
# Collection Settings
# ============================================================================
collection:
  # What to collect
  collect_tables: true
  collect_volumes: true
  collect_sizes: true    # Collect table sizes (requires warehouse_id)
  
  # Size collection tuning
  size_workers: 20       # Parallel workers for SQL queries (Tier 2)
  size_threshold: 200    # Table count threshold for Spark job (Tier 3)
  max_parallel_spark_jobs: 3  # Max concurrent Spark jobs

# ============================================================================
# Logging
# ============================================================================
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
